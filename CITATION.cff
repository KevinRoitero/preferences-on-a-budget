@proceedings{10.1145/3485447.3511960,
author = {Roitero, Kevin and Checco, Alessandro and Mizzaro, Stefano and Demartini, Gianluca},
title = {Preferences on a Budget: Prioritizing Document Pairs When Crowdsourcing Relevance Judgments},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511960},
doi = {10.1145/3485447.3511960},
abstract = { In Information Retrieval (IR) evaluation, preference judgments are collected by presenting to the assessors a pair of documents and asking them to select which of the two, if any, is the most relevant. This is an alternative to the classic relevance judgment approach, in which human assessors judge the relevance of a single document on a scale; such an alternative allows to make relative rather than absolute judgments of relevance. While preference judgments are easier for human assessors to perform, the number of possible document pairs to be judged is usually so high that it makes it unfeasible to judge them all. Thus, following a similar idea to pooling strategies for single document relevance judgments where the goal is to sample the most useful documents to be judged, in this work we focus on analyzing alternative ways to sample document pairs to judge, in order to maximize the value of a fixed number of preference judgments that can feasibly be collected. Such value is defined as how well we can evaluate IR systems given a budget, that is, a fixed number of human preference judgments that may be collected. By relying on several datasets featuring relevance judgments gathered by means of experts and crowdsourcing, we experimentally compare alternative strategies to select document pairs and show how different strategies lead to different IR evaluation result quality levels. Our results show that, by using the appropriate procedure, it is possible to achieve good IR evaluation results with a limited number of preference judgments, thus confirming the feasibility of using preference judgments to create IR evaluation collections.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {319â€“327},
numpages = {9},
keywords = {Crowdsourcing, Preference Judgments, Relevance Assessment},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}
