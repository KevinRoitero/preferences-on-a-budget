{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "\n",
    "def makedir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def majority_vote(x):\n",
    "    c = Counter(x)\n",
    "    value, count = c.most_common()[0]\n",
    "    return value\n",
    "def nothing(x):\n",
    "    if 1 != len(np.unique(x.values)):\n",
    "        assert False, \"error on aggregation\"\n",
    "    else:\n",
    "        return x.values[0]\n",
    "import itertools\n",
    "    \n",
    "topics_to_consider = ['402', '403', '405', '407', '408', '410', '415', '416', '418', '420', '421', '427', '428', '431', '440', '442', '445', '448']\n",
    "topics_to_consider_moffat = ['402', '403', '405', '407', '408', '415', '416', '431', '440']\n",
    "#topics_to_consider = topics_to_consider_moffat[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=4)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def makedir(PATH):\n",
    "    from pathlib import Path\n",
    "    Path(PATH).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "import xml.etree.ElementTree\n",
    "\n",
    "topics = '../../src/topics.xml' # <- File from which the inforamtion are taken\n",
    "e = xml.etree.ElementTree.parse(topics).getroot()\n",
    "gold_dict = {}\n",
    "    \n",
    "for child in e:\n",
    "    topic = child.attrib[\"id\"]    \n",
    "    gold_dict[topic,\"N\"] = child.find('N').text\n",
    "    gold_dict[topic,\"H\"] = child.find('H').text\n",
    "    \n",
    "assert len(gold_dict), \"Gold should be 36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' do the qrels '''\n",
    "DATASET = 'ME'\n",
    "df = pd.read_csv(f'../../src/crowd-data/{DATASET}.csv').reset_index()\n",
    "#df[f'{DATASET}_rel'] = df[f'{DATASET}_rel'].astype(int)\n",
    "df = df[df[\"doc_id\"].isin(gold_dict.values()) == False]\n",
    "df = df[['topic', 'doc_id', f'{DATASET}_rel', 'rel_TREC', 'rel_Sormunen']].groupby(['topic', 'doc_id']).agg('mean').reset_index()\n",
    "df['zero'] = 0\n",
    "df = df[['topic', 'zero', 'doc_id', f'{DATASET}_rel']]\n",
    "topics = np.unique(df['topic'])\n",
    "\n",
    "df.to_csv(f'./pers_script/dataset-evaluation/qrels_{DATASET}.txt', header=False, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' do the qrels for moffat '''\n",
    "DD = 'ratio'\n",
    "#DD = 'rele'\n",
    "#DD = 'pref'\n",
    "DATASET = f'moffat{DD}'\n",
    "df = pd.read_csv(f'../../../AAA_DATASET/Moffat-and-co/OneDrive_1_25-11-2020/result_files/qrel_pref+rele+ratio.csv').reset_index()\n",
    "df = df[df[\"doc\"].isin(gold_dict.values()) == False]\n",
    "df = df[['topic', 'doc', f'{DD}']]#.groupby(['topic', 'doc']).agg('mean').reset_index()\n",
    "df['zero'] = 0\n",
    "df = df[['topic', 'zero', 'doc', f'{DD}']]\n",
    "df[f'doc'] = df[f'doc'].astype(str)\n",
    "df[f'doc'] = [x.strip() for x in df[f'doc']]\n",
    "df[f'{DD}'] = np.round(df[f'{DD}'],2)\n",
    "df[f'{DD}'] *= 100\n",
    "\n",
    "topics = np.unique(df['topic'])\n",
    "df.to_csv(f'./pers_script/dataset-evaluation/qrels_{DATASET}.txt', header=False, index=False, sep=' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' do the qrels for S100toPref '''\n",
    "DATASET = 'S4toPref'\n",
    "df = pd.read_csv(f'../../src/S100toPreference/S4_pairwise_full_scores.csv')\n",
    "df = df[df[\"document\"].isin(gold_dict.values()) == False]\n",
    "df = df[['topic', 'document', f'score']].groupby(['topic', 'document']).agg('mean').reset_index()\n",
    "df['zero'] = 0\n",
    "df = df[['topic', 'zero', 'document', f'score']]\n",
    "topics = np.unique(df['topic'])\n",
    "\n",
    "df.to_csv(f'./pers_script/dataset-evaluation/qrels_{DATASET}.txt', header=False, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"--- run trec_eval  --- \"\"\"\n",
    "import os\n",
    "\n",
    "#for DATASET in tqdm([f'S100','ME','S100toPref','MEtoPref','moffatratio'], desc=\"DATASET loop\", leave=True):\n",
    "for DATASET in tqdm([f'S2','S4','S2toPref','S4toPref'], desc=\"DATASET loop\", leave=True):\n",
    "\n",
    "    QRELS_PATH = f'./pers_script/dataset-evaluation/qrels_{DATASET}.txt'\n",
    "    OFFICIAL_QRELS_PATH = '../../../AAA_TREC-DATA/TREC-qrels/qrels.AH99.txt'\n",
    "    UNCOMPRESSED_PATH = '../../../AAA_TREC-DATA/Uncompressed/AH99/input/'\n",
    "\n",
    "    files = [f for f in listdir(UNCOMPRESSED_PATH) if (isfile(join(UNCOMPRESSED_PATH, f))) & (f != \".DS_Store\")]\n",
    "    runs = [f.replace('input.','') for f in files]\n",
    "\n",
    "    for k in tqdm([2,3,4,5,6,7,8,9,10], desc=\"K loop\", leave=False):\n",
    "        SAVE_PATH = f'./pers_script/dataset-evaluation/eval_{k}/'\n",
    "        makedir(f'{SAVE_PATH}/official')\n",
    "        makedir(f'{SAVE_PATH}/{DATASET}')\n",
    "        for run in tqdm(runs, desc=\"run loop\", leave=False):\n",
    "\n",
    "            os.system(f\"trec_eval -q -m ndcg -M {k} {OFFICIAL_QRELS_PATH} {UNCOMPRESSED_PATH}/input.{run} > ./{SAVE_PATH}/official/{run}.txt \")\n",
    "            os.system(f\"trec_eval -q -m ndcg -M {k} {QRELS_PATH} {UNCOMPRESSED_PATH}/input.{run} > ./{SAVE_PATH}/{DATASET}/{run}.txt \")\n",
    "\n",
    "            #assert False\n",
    "print('\\t done computing teval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"--- evaluate (also multiple datasets)  --- \"\"\"\n",
    "\n",
    "df_ev = pd.DataFrame(columns=['kind','topic', 'system', 'metric','score'])\n",
    "\n",
    "def parses(path, kind, k):\n",
    "    df_ev = pd.DataFrame(columns=['kind','topic', 'system', 'metric','score'])\n",
    "    RUN_PATH = path\n",
    "    files = [f for f in listdir(RUN_PATH) if (isfile(join(RUN_PATH, f))) & (f != \".DS_Store\")]\n",
    "    for i, f in enumerate(files):\n",
    "        sys = f.replace('summary.{}.input.'.format(kind), '').replace('.txt', '')\n",
    "        df =  pd.read_csv(join(RUN_PATH, f), sep='\\t', header=None)\n",
    "        df.columns = ['metric', 'topic', 'score']\n",
    "        df['metric'] = [x.strip() for x in df['metric']]\n",
    "        df = df[df['metric']=='ndcg']\n",
    "        df = df[df['topic']!='all']\n",
    "        df['kind'] = kind\n",
    "        df['metric'] = f'ndcg@{k}'\n",
    "        df['system'] = sys\n",
    "               \n",
    "        df_ev = pd.concat([df_ev, df])\n",
    "    print(f\"done for {kind}--{k}\")\n",
    "    return df_ev\n",
    "\n",
    "SAVE_PATH = f'./pers_script/dataset-evaluation/'\n",
    "df_eval = pd.concat([\n",
    "parses(f'./{SAVE_PATH}/eval_2/official/', 'TREC8', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/official/', 'TREC8', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/official/', 'TREC8', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/official/', 'TREC8', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/official/', 'TREC8', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/official/', 'TREC8', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/official/', 'TREC8', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/official/', 'TREC8', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/official/', 'TREC8', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/S2/', f'S2', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/S2/', f'S2', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/S2/', f'S2', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/S2/', f'S2', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/S2/', f'S2', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/S2/', f'S2', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/S2/', f'S2', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/S2/', f'S2', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/S2/', f'S2', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/S4/', f'S4', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/S4/', f'S4', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/S4/', f'S4', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/S4/', f'S4', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/S4/', f'S4', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/S4/', f'S4', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/S4/', f'S4', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/S4/', f'S4', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/S4/', f'S4', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/S100/', f'S100', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/S100/', f'S100', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/S100/', f'S100', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/S100/', f'S100', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/S100/', f'S100', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/S100/', f'S100', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/S100/', f'S100', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/S100/', f'S100', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/S100/', f'S100', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/ME/', f'ME', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/ME/', f'ME', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/ME/', f'ME', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/ME/', f'ME', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/ME/', f'ME', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/ME/', f'ME', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/ME/', f'ME', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/ME/', f'ME', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/ME/', f'ME', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/moffatratio/', f'moffatratio', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/moffatratio/', f'moffatratio', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/moffatratio/', f'moffatratio', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/moffatratio/', f'moffatratio', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/moffatratio/', f'moffatratio', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/moffatratio/', f'moffatratio', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/moffatratio/', f'moffatratio', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/moffatratio/', f'moffatratio', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/moffatratio/', f'moffatratio', 10),\n",
    "#\n",
    "#parses(f'./{SAVE_PATH}/eval_3/moffatrele/', f'moffatrele', 3),\n",
    "#parses(f'./{SAVE_PATH}/eval_5/moffatrele/', f'moffatrele', 5),\n",
    "#parses(f'./{SAVE_PATH}/eval_10/moffatrele/', f'moffatrele', 10),\n",
    "# #\n",
    "#parses(f'./{SAVE_PATH}/eval_3/moffatpref/', f'moffatpref', 3),\n",
    "#parses(f'./{SAVE_PATH}/eval_5/moffatpref/', f'moffatpref', 5),\n",
    "#parses(f'./{SAVE_PATH}/eval_10/moffatpref/', f'moffatpref', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/S100toPref/', f'S100toPref', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/S100toPref/', f'S100toPref', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/S100toPref/', f'S100toPref', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/S100toPref/', f'S100toPref', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/S100toPref/', f'S100toPref', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/S100toPref/', f'S100toPref', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/S100toPref/', f'S100toPref', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/S100toPref/', f'S100toPref', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/S100toPref/', f'S100toPref', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/MEtoPref/', f'MEtoPref', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/MEtoPref/', f'MEtoPref', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/MEtoPref/', f'MEtoPref', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/MEtoPref/', f'MEtoPref', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/MEtoPref/', f'MEtoPref', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/MEtoPref/', f'MEtoPref', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/MEtoPref/', f'MEtoPref', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/MEtoPref/', f'MEtoPref', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/MEtoPref/', f'MEtoPref', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/S2toPref/', f'S2toPref', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/S2toPref/', f'S2toPref', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/S2toPref/', f'S2toPref', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/S2toPref/', f'S2toPref', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/S2toPref/', f'S2toPref', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/S2toPref/', f'S2toPref', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/S2toPref/', f'S2toPref', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/S2toPref/', f'S2toPref', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/S2toPref/', f'S2toPref', 10),\n",
    "#\n",
    "parses(f'./{SAVE_PATH}/eval_2/S4toPref/', f'S4toPref', 2),\n",
    "parses(f'./{SAVE_PATH}/eval_3/S4toPref/', f'S4toPref', 3),\n",
    "parses(f'./{SAVE_PATH}/eval_4/S4toPref/', f'S4toPref', 4),\n",
    "parses(f'./{SAVE_PATH}/eval_5/S4toPref/', f'S4toPref', 5),\n",
    "parses(f'./{SAVE_PATH}/eval_6/S4toPref/', f'S4toPref', 6),\n",
    "parses(f'./{SAVE_PATH}/eval_7/S4toPref/', f'S4toPref', 7),\n",
    "parses(f'./{SAVE_PATH}/eval_8/S4toPref/', f'S4toPref', 8),\n",
    "parses(f'./{SAVE_PATH}/eval_9/S4toPref/', f'S4toPref', 9),\n",
    "parses(f'./{SAVE_PATH}/eval_10/S4toPref/', f'S4toPref', 10),\n",
    "])\n",
    "df_eval['topic'] = df_eval['topic'].astype(str)\n",
    "df_eval = df_eval[df_eval['topic'].isin(topics_to_consider)]\n",
    "df_eval['score'] = df_eval['score'].astype(float)\n",
    "\n",
    "df_eval.to_csv(f'./pers_script/dataset-evaluation/evaluation-results.txt', header=True, index=False, sep=',')\n",
    "\n",
    "display(df_eval.head())\n",
    "display(df_eval.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' compute correlations '''\n",
    "from tauAP import tauAP, rank\n",
    "import scipy.stats\n",
    "\n",
    "df_eval = pd.read_csv(f'./pers_script/dataset-evaluation/evaluation-results.txt', sep=',')\n",
    "\n",
    "print(np.unique(df_eval['kind']))\n",
    "print(np.unique(df_eval['topic']))\n",
    "print(np.unique(df_eval['metric']))\n",
    "\n",
    "df_plot = df_eval.groupby(['kind', 'system','metric']).agg('mean').reset_index()\n",
    "ddres = pd.DataFrame(columns=['metric','x','y','rho','tau','tauAP','tau10'])\n",
    "\n",
    "for metric in ['ndcg@2','ndcg@3','ndcg@4','ndcg@5','ndcg@6','ndcg@7','ndcg@8','ndcg@9','ndcg@10']:\n",
    "    sub = df_plot[df_plot['metric']==metric]\n",
    "\n",
    "    ords = [('TREC8',f'S100'),('TREC8',f'ME'),('TREC8',f'moffatratio'),('TREC8',f'S100toPref'),('TREC8',f'MEtoPref'),\n",
    "           ('TREC8',f'S2'),('TREC8',f'S4'),('TREC8',f'S2toPref'),('TREC8',f'S4toPref')]\n",
    "    #ords = [('TREC8',f'S100')]\n",
    "    for i, (x_val, y_val) in enumerate(ords):\n",
    "#         print(x_val,y_val)\n",
    "#         print(sub[sub['kind']==y_val])\n",
    "        x = sub[sub['kind']==x_val]['score'].values \n",
    "        y = sub[sub['kind']==y_val]['score'].values\n",
    "        \n",
    "        rho, rhoval = sp.stats.pearsonr(x,y)\n",
    "        tau, tauval = sp.stats.kendalltau(x,y)\n",
    "\n",
    "        tauap = tauAP(x,y, top_heavy=True)\n",
    "\n",
    "        xten = x\n",
    "        yten = y\n",
    "        xten, yten = zip(*sorted(zip(xten, yten)))\n",
    "        xten = list(xten[::-1])\n",
    "        yten = list(yten[::-1])\n",
    "        xten = xten[:10]\n",
    "        yten = yten[:10]\n",
    "        tauten, tauvalten = sp.stats.kendalltau(xten,yten)      \n",
    "\n",
    "        ddres.loc[len(ddres)] = [metric,x_val, y_val, rho, tau, tauap, tauten]\n",
    "            \n",
    "display(ddres)\n",
    "ddres.to_csv(f'./pers_script/dataset-evaluation/correlations-all-topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' compute correlations '''\n",
    "from tauAP import tauAP, rank\n",
    "import scipy.stats\n",
    "\n",
    "df_eval = pd.read_csv(f'./pers_script/dataset-evaluation/evaluation-results.txt', sep=',')\n",
    "\n",
    "df_plot = df_eval[df_eval['topic'].isin(topics_to_consider_moffat)].groupby(['kind', 'system','metric']).agg('mean').reset_index()\n",
    "ddres = pd.DataFrame(columns=['metric','x','y','rho','tau','tauAP','tau10'])\n",
    "\n",
    "for metric in ['ndcg@3','ndcg@5','ndcg@10']:\n",
    "    sub = df_plot[df_plot['metric']==metric]\n",
    "\n",
    "    ords = [('TREC8',f'S100'),('TREC8',f'ME'),('TREC8',f'S2'),('TREC8',f'S4'),('TREC8',f'moffatratio'),\n",
    "           ('TREC8',f'S2'),('TREC8',f'S4'),('TREC8',f'S2toPref'),('TREC8',f'S4toPref')]\n",
    "    for i, (x_val, y_val) in enumerate(ords):\n",
    "        print(x_val,y_val)\n",
    "        x = sub[sub['kind']==x_val]['score'].values \n",
    "        y = sub[sub['kind']==y_val]['score'].values\n",
    "        \n",
    "        rho, rhoval = sp.stats.pearsonr(x,y)\n",
    "        tau, tauval = sp.stats.kendalltau(x,y)\n",
    "\n",
    "        tauap = tauAP(x,y, top_heavy=True)\n",
    "\n",
    "        xten = x\n",
    "        yten = y\n",
    "        xten, yten = zip(*sorted(zip(xten, yten)))\n",
    "        xten = list(xten[::-1])\n",
    "        yten = list(yten[::-1])\n",
    "        xten = xten[:10]\n",
    "        yten = yten[:10]\n",
    "        tauten, tauvalten = sp.stats.kendalltau(xten,yten)      \n",
    "\n",
    "        ddres.loc[len(ddres)] = [metric,x_val, y_val, rho, tau, tauap, tauten]\n",
    "            \n",
    "display(ddres)\n",
    "ddres.to_csv(f'./pers_script/dataset-evaluation/correlations-moffat-topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
